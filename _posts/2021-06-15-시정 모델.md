```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.model_selection import KFold
from math import sqrt
from sklearn.metrics import mean_squared_error
from torch.utils.data import Dataset

import os
import glob
import time
import datetime
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
%matplotlib inline
from itertools import product

from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

torch.manual_seed(0);
```


```python
# Hyper Parameters

obs_point = 921
region = 'bu'
vis_limit_list = [1000, 3000,5000,10000,30000]

num_unit_1_list = [256]
num_unit_2_list = [256]
num_unit_3_list = [64]
lr_list = [0.0004]
batch_size_list = [32]
epochs_list = [300,800,1300,1800]
k_fold_list = [5]

data_path = 'D:\\Onedrive\\CNU\\OneDrive - 충남대학교\\Coding\\LAB\\Lab\\preprocessing_templete(for fog data)\\data\\new_fog_data\\' + str(obs_point)
files = glob.glob(data_path + '\\*.csv')

def parameters_list(num_unit_1_list, num_unit_2_list, num_unit_3_list, lr_list, batch_size_list, epochs_list, k_fold_list):
    items = [num_unit_1_list, num_unit_2_list, num_unit_3_list, lr_list, batch_size_list, epochs_list, k_fold_list]
    result = list(product(*items))
    return result

all_parameters_list = parameters_list(num_unit_1_list, num_unit_2_list, num_unit_3_list, lr_list, batch_size_list, epochs_list, k_fold_list)
```

# DNN Network Clss


```python
class Deep_Neural_Network(nn.Module):
    def __init__(self, num_unit_1 = 200, num_unit_2 = 70, num_unit_3 = 20, drop_out=0):
        super(Deep_Neural_Network, self).__init__()
        self.fc1 = nn.Linear(9, num_unit_1)
        self.fc2 = nn.Linear(num_unit_1, num_unit_1)
        self.fc3 = nn.Linear(num_unit_1, num_unit_2)
        self.fc4 = nn.Linear(num_unit_2, num_unit_3)
        self.output = nn.Linear(num_unit_3, 1)
        self.dropout = nn.Dropout(drop_out)

    def forward(self, x, **kwargs):
        fc1 = self.dropout(F.leaky_relu(self.fc1(x), negative_slope=0.2))
        fc2 = self.dropout(F.leaky_relu(self.fc2(fc1), negative_slope=0.2))
        fc3 = self.dropout(F.leaky_relu(self.fc3(fc2), negative_slope=0.2))
        fc4 = self.dropout(F.leaky_relu(self.fc4(fc3), negative_slope=0.2))
        fc5 = self.output(fc4)
        return fc5
```

# Train Function


```python
def train(model, device, n_epochs, optimizer, loss_fn, batch_size, n_fold, kfold, torch_train_x, torch_train_y, torch_test_x, torch_test_y, save_Model=False):
    total_acc = 0
    per_proceed = 0
    for fold, (train_index, test_index) in enumerate(kfold.split(torch_train_x, torch_train_y)):
        ### Dividing data into folds
        x_train_fold = torch_train_x[train_index]
        x_test_fold = torch_train_x[test_index]
        y_train_fold = torch_train_y[train_index]
        y_test_fold = torch_train_y[test_index]

        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)
        test = torch.utils.data.TensorDataset(x_test_fold, y_test_fold)
        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)
        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)

        for epoch in range(n_epochs):
            # print('\nEpoch {} / {} \nFold number {} / {}'.format(epoch + 1, epochs, fold + 1 , kfold.get_n_splits()))
            # network.train()
            for batch_index, (x_batch, y_batch) in enumerate(train_loader):
                x_batch, y_batch = x_batch.to(device), y_batch.to(device)
                optimizer.zero_grad()
                # print(x_batch.shape)
                out = model(x_batch)
                y_batch = y_batch.reshape([y_batch.shape[0],1])
                # print(out.shape, y_batch.shape)
                loss = loss_fn(out, y_batch)
                # loss = torch.tensor(torch.round(loss), dtype=torch.float, requires_grad=True)
                # print(loss)
                loss.backward()
                optimizer.step()

            train_r2_score = r2_score(np.array(torch_train_y), model(torch_train_x.cuda()).cpu().detach().numpy())
            train_r_score = return_corr(model, torch_train_x, torch_train_y)
            train_df = pd.DataFrame({'Target':np.array(torch_train_y),'Model':model(torch_train_x.cuda()).cpu().detach().numpy().reshape(-1)})
            train_rmse = sqrt(mean_squared_error(np.array(torch_train_y), model(torch_train_x.cuda()).cpu().detach().numpy()))

            test_r2_score = r2_score(np.array(torch_test_y), model(torch_test_x.cuda()).cpu().detach().numpy())
            test_r_score = return_corr(model, torch_test_x, torch_test_y)
            test_df = pd.DataFrame({'Target':np.array(torch_test_y),'Model':model(torch_test_x.cuda()).cpu().detach().numpy().reshape(-1)})
            test_rmse = sqrt(mean_squared_error(np.array(torch_test_y), model(torch_test_x.cuda()).cpu().detach().numpy()))

            total_result_df = pd.concat([train_df, test_df], axis = 1)
            if epoch % 2 == 0:
                per_proceed += 50
                print(" Fold : {}, Epoch : {}\ntrain r2 score : {:.3f}, train r score : {:.3f}, train_rmse : {:.2f}\ntest r2 score : {:.3f}, test r score : {:.3f}, test_rmse : {:.2f}\n\n".
                      format(fold + 1, epoch + 1, train_r2_score, train_r_score, train_rmse,test_r2_score, test_r_score, test_rmse))
                # print(total_result_df.head(10),'\n\n')

        vali_r2_score = 0
        for vali_x_batch, vali_y_batch in test_loader:
            # print(vali_x_batch.shape, vali_y_batch.shape)
            vali_x_batch = vali_x_batch.to(device)
            vali_out = model(vali_x_batch)
            r2 = r2_score(vali_y_batch, vali_out.cpu().detach().numpy())
            vali_r2_score += r2

        print("vali_r2_score is {:.3f}".format(vali_r2_score / len(test_loader)))
    test_rmse = sqrt(mean_squared_error(np.array(torch_test_y), model(torch_test_x.cuda()).cpu().detach().numpy()))

    print("Fold : {}, Epoch : {}\n train r2 score : {:.3f}, train r score : {:.3f}, train_rmse : {:.2f}\ntest r2 score : {:.3f}, test r score : {:.3f}, test_rmse : {:.2f}\n\n".
          format(fold + 1, epoch + 1, train_r2_score, train_r_score, train_rmse, test_r2_score, test_r_score, test_rmse))

    return model
```

# Preprocessing Function


```python
def minmax_func(df):
    # 타겟값 및 이미 sin변환한 데이터를 제외하고 표준화 하기
    normal_columns = df.columns.difference(['Local Time','DOY','vis'])
    minmax_df = df[normal_columns]
    scaler = MinMaxScaler(); scaler.fit(minmax_df)
    minmax_columns = 'stand_' + minmax_df.columns
    minmax_df = scaler.transform(minmax_df)
    minmax_df = pd.DataFrame(minmax_df, columns=minmax_columns)
    minmax_df = pd.concat([minmax_df, df[['Local Time','DOY','vis']].reset_index().drop(['index'],axis=1)], axis=1)
    return minmax_df


def doy_local_to_sin(data_path):
    result_pd = pd.read_csv(data_path)

    # 로컬타임 사인으로 변환한후 더하기
    local_time = result_pd['hour'] + result_pd['minute']/60
    sin_local_time = (np.sin((local_time*2*np.pi)/24))
    result_pd['Local Time'] = sin_local_time

    if result_pd['year'][0] % 4 == 0:
        result_pd['DOY'] = leap_doy
    else:
        result_pd['DOY'] = normal_doy
    return result_pd

def return_corr(model, torch_test_x, torch_test_y):
    r2_pred = model(torch_test_x.cuda()).cpu().detach().numpy()
    r2_target = np.array(torch_test_y)
    df_r2 = pd.DataFrame()
    df_r2['r2_pred'] = r2_pred[:,0]
    df_r2['r2_target'] = r2_target
    return df_r2.corr().iloc[0,1]


# oversample function
def number_of_data(df, kind_of_data, min_num, max_num):
    cond1 = df[kind_of_data] > min_num
    cond2 = df[kind_of_data] <= max_num
    return df[cond1 & cond2]

# oversample function
def append_same_data(df, time):
    result_df = pd.DataFrame()
    for i in range(time):
        result_df = result_df.append(df)
    return result_df

def convert_torch_type(input_train_data, target_train_data, input_test_data, target_test_data):
    torch_train_x = torch.tensor(np.array(input_train_data), dtype=torch.float32)
    torch_train_y = torch.tensor(np.array(target_train_data), dtype=torch.float32)
    torch_test_x = torch.tensor(np.array(input_test_data), dtype=torch.float32)
    torch_test_y = torch.tensor(np.array(target_test_data), dtype=torch.float32)
    return torch_train_x, torch_train_y, torch_test_x, torch_test_y
```

## (Preprocessing) Convert Day Of Year, Local time to sin

### - result_pd == total_pd


```python
leap_month_list = [31,29,31,30,31,30,31,31,30,31,30,31]
normal_month_list = [31,28,31,30,31,30,31,31,30,31,30,31]
leap_year_list = []
normal_year_list = []
for i in range(sum(leap_month_list)):
    temp = [i+1]*144
    leap_year_list += temp
    
for i in range(sum(normal_month_list)):
    temp = [i+1]*144
    normal_year_list += temp
    
leap_doy = np.sin((np.array(leap_year_list)*(2*np.pi))/366)
normal_doy = np.sin((np.array(normal_year_list)*(2*np.pi))/365)

for i in range(len(files)):
    if i == 0:
        total_pd = pd.read_csv(files[i])
        
        # 로컬타임 사인으로 변환한후 더하기
        local_time = total_pd['hour'] + total_pd['minute']/60
        sin_local_time = (np.sin((local_time*2*np.pi)/24))
        total_pd['Local Time'] = sin_local_time
        
        if total_pd['year'][0] % 4 == 0:
            total_pd['DOY'] = leap_doy
        else:
            total_pd['DOY'] = normal_doy
            
        
        
    else:
        temp_pd = pd.read_csv(files[i])
        
        # 로컬타임 사인으로 변환한후 더하기
        local_time = temp_pd['hour'] + temp_pd['minute']/60
        sin_local_time = (np.sin((local_time*2*np.pi)/24))
        temp_pd['Local Time'] = sin_local_time
        
        if temp_pd['year'][0] % 4 == 0:
            temp_pd['DOY'] = leap_doy
        else:
            temp_pd['DOY'] = normal_doy
        
        
        total_pd = pd.concat([total_pd, temp_pd], axis = 0)
```

## Import data & Drop useless data


```python
if region == 'bu':
    # train_pd.drop(['Unnamed: 0','year','month','day','hour','minute','Fog_10','LST','WD','RH','WS','Td','AT','WT-AT'], axis=1, inplace = True)
    total_pd.drop(['Unnamed: 0','year','month','day','hour','minute','Fog_10','LST','bWD','bRH','bWS','bTd','bAT','WT-AT'], axis=1, inplace = True)
    total_pd.replace(-999, np.nan, inplace = True)
    total_pd.columns = ['Fog_30', 'ww', 'vis', 'RN', 'WT', 'AT', 'RH', 'WS', 'WD', 'Td', 'AT-3hr', 'Local Time', 'DOY']

elif region == 'se':
    total_pd.drop(['Unnamed: 0','year','month','day','hour','minute','Fog_10','LST','WT-AT'], axis=1, inplace = True)
    total_pd.replace(-999, np.nan, inplace = True)

```


```python
## Remove null & Calculate 'WT-AT'
```


```python
# Remove null value
total_data = total_pd.dropna()
print(total_data.shape)


# 'WT-AT'
total_data['WT-AT'] = total_data['WT'] - total_data['AT']
```

# Choose method(only one)

### 1st method : remove non occurrence fog data

### 2nd method : limit visibility
- 3000 -> 64
- 5000 -> 128
- 10000 -> 256
- 30000 -> 2048


```python
# 1st method
total_df = total_data[total_data['Fog_30'] == 1]
```


```python
# 2nd method
# vis_limit_num = 5000
# total_df = total_data[total_data['vis'] < vis_limit_num]
```

## Split data for train, test


```python
# train test split
X_train_columns = scaled_total_df.columns.difference(['stand_RN','stand_AT-3hr','stand_ww',
                                                     'stand_Fog_30','vis'])

input_data = scaled_total_df[X_train_columns]
train_x, test_x, train_y, test_y = train_test_split(input_data, (scaled_total_df['vis']).astype(np.int),
                                    test_size = 0.3, random_state=42)
print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)
```

## array to torch(for train)


```python
# change to torch type
torch_train_x, torch_train_y, torch_test_x, torch_test_y = convert_torch_type(train_x, train_y, test_x, test_y)
torch_train_x = torch_train_x.reshape([torch_train_x.shape[0], torch_train_x.shape[1]])
torch_test_x = torch_test_x.reshape([torch_test_x.shape[0], torch_test_x.shape[1]])
print(torch_train_x.shape, torch_train_y.shape, torch_test_x.shape, torch_test_y.shape)
```

## Train Code


```python
train_r_result = []
train_r2_result = []
train_rmse_result = []
test_r_result = []
test_r2_result = []
test_rmse_result = []
used_time_list = []

for i in range(len(all_parameters_list)):
    print("진행률 : {}%".format(((i + 1) / len(all_parameters_list)) * 100))
    start = time.time()
    num_unit_1, num_unit_2, num_unit_3, lr, batch_size, n_epochs, n_fold,  = all_parameters_list[i][0], \
                                                                                      all_parameters_list[i][1], \
                                                                                      all_parameters_list[i][2], \
                                                                                      all_parameters_list[i][3], \
                                                                                      all_parameters_list[i][4], \
                                                                                      all_parameters_list[i][5], \
                                                                                      all_parameters_list[i][6],
    seed = 1
    lr = lr
    momentum = 0.5
    no_cuda = False
    batch_size = batch_size
    # 배치사이즈와 모멘텀등 몇가지 변수들을 지정해줍니다.
    torch.manual_seed(seed)
    use_cuda = not no_cuda and torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    model = Deep_Neural_Network(num_unit_1=num_unit_1, num_unit_2=num_unit_2, num_unit_3=num_unit_3).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    loss_fn = torch.nn.MSELoss(reduction='sum')
    kfold = KFold(n_splits=n_fold, shuffle=False)
    network = train(model, device, n_epochs, optimizer, loss_fn, batch_size, n_fold, kfold, torch_train_x, torch_train_y, torch_test_x, torch_test_y)


    used_time = time.time() - start
    used_time_list.append(used_time)
    print("현재환경", num_unit_1, num_unit_2, num_unit_3, lr, batch_size, n_epochs, n_fold)
    print("총 걸린시간은 {}초 입니다.\n".format(used_time))
    
    
    # train, test DataFrame
    train_df = pd.DataFrame({'Target': np.array(torch_train_y),
                             'Model': network(torch_train_x.cuda()).cpu().detach().numpy().reshape(-1)})
    
    
    test_df = pd.DataFrame({'Target': np.array(torch_test_y),
                            'Model': network(torch_test_x.cuda()).cpu().detach().numpy().reshape(-1)})
    
    # calculate model result
    train_r_score = np.array(train_df.corr())[0][1]
    test_r_score = np.array(test_df.corr())[0][1]
    train_r2_score = r2_score(np.array(torch_train_y), network(torch_train_x.cuda()).cpu().detach().numpy().reshape(-1))
    test_r2_score = r2_score(np.array(torch_test_y), network(torch_test_x.cuda()).cpu().detach().numpy().reshape(-1))
    train_rmse = sqrt(mean_squared_error(np.array(torch_train_y), model(torch_train_x.cuda()).cpu().detach().numpy()))
    test_rmse = sqrt(mean_squared_error(np.array(torch_test_y), model(torch_test_x.cuda()).cpu().detach().numpy()))
    
    train_r_result.append(train_r_score)
    test_r_result.append(test_r_score)
    train_r2_result.append(train_r2_score)
    test_r2_result.append(test_r2_score)
    train_rmse_result.append(train_rmse)
    test_rmse_result.append(test_rmse)
    
    
    # Drawing Scatter Plot
    x = train_df['Target']
    y = train_df['Model']
    z = np.polyfit(x,y,1)
    p = np.poly1d(z)

    plt.rcParams['font.family'] = "Malgun Gothic"
    plt.rcParams['font.size'] = 30
    plt.rcParams['figure.figsize'] = (20, 20)
    pylab.plot(x,y,'o')
    pylab.plot(x,p(x),"r--", linewidth = 3)
    plt.title('(Train) '+str(obs_point)+' under {}'.format(vis_limit_num), pad = 30, fontdict={'size':40})
    plt.xlim(0,vis_limit_num)
    plt.ylim(0,vis_limit_num)
    plt.xlabel("(Train) Target")
    plt.ylabel("(Train) Model")
    plt.text(vis_limit_num//50, vis_limit_num + vis_limit_num//50, 'R : {:.3f} / R2 : {:.3f}'.format(train_r_score, train_r2_score), bbox=box1, fontdict = font2)
    plt.text(vis_limit_num//50, vis_limit_num - vis_limit_num//30, "y = %.3fx + (%.3f)"%(z[0],z[1]), bbox=box1, fontdict = font2)
    plt.savefig('D:\\result\\fog_result\\DNN\\'+
            'Train_{}_under_{}_{}_{}_{}_{}_{}_{}_{}.jpg'.format(obs_point,vis_limit_num, num_unit_1, num_unit_2, num_unit_3, lr, batch_size, n_epochs, n_fold), dpi = 300)
    plt.show()
    plt.clf()

    
    x = test_df['Target']
    y = test_df['Model']
    z = np.polyfit(x,y,1)
    p = np.poly1d(z)

    plt.rcParams['font.family'] = "Malgun Gothic"
    plt.rcParams['font.size'] = 30
    plt.rcParams['figure.figsize'] = (20, 20)
    pylab.plot(x,y,'o')
    pylab.plot(x,p(x),"r--", linewidth = 3)
    plt.title('(Test) '+str(obs_point)+' under {}'.format(vis_limit_num), pad = 30, fontdict={'size':40})
    plt.xlim(0,vis_limit_num)
    plt.ylim(0,vis_limit_num)
    plt.xlabel("(Test) Target")
    plt.ylabel("(Test) Model")    
    plt.text(vis_limit_num//50, vis_limit_num + vis_limit_num//50, 'R : {:.3f} / R2 : {:.3f}'.format(test_r_score, test_r2_score), bbox=box1, fontdict = font2)
    plt.text(vis_limit_num//50, vis_limit_num - vis_limit_num//30, "y = %.3fx + (%.3f)"%(z[0],z[1]), bbox=box1, fontdict = font2)
    plt.savefig('D:\\result\\fog_result\\DNN\\'+
                'Test_{}_under_{}_{}_{}_{}_{}_{}_{}_{}.jpg'.format(obs_point,vis_limit_num, num_unit_1, num_unit_2, num_unit_3, lr, batch_size, n_epochs, n_fold), dpi = 300)
    plt.show()
    plt.clf()
```

## Save Model Result


```python
result_df = pd.DataFrame({'Train R':train_r_result, 'Train R2':train_r2_result, 'Train RMSE':train_rmse_result,'Test R':test_r_result,'Test R2':test_r2_result,'Test RMSE':test_rmse_result})
columns = ['layer_1', 'layer_2', 'layer_3', 'lr', 'bs', 'epochs', 'kfold']
nn_structure = pd.DataFrame(all_com_list, columns=columns)
result_df = pd.concat([result_df, nn_structure], axis=1)
sort_result_df = result_df.sort_values('Test R2', ascending=False)
file_name = str(obs_point) + "_minmax_random_train_exist_3" + ".csv"
# sort_result_df.to_csv('D:\\Onedrive\\CNU\\OneDrive - 충남대학교\\LAB\\과제\\스마트시티 안개\\결과정리\\상관관계 및 DNN 결과\\DNN, VISIBILITY MODEL\\' + file_name)
```
