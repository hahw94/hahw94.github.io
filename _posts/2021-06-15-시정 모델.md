```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from sklearn.model_selection import KFold
from math import sqrt
from sklearn.metrics import mean_squared_error
from torch.utils.data import Dataset

import os
import glob
import time
import datetime
import pandas as pd
import numpy as np
import seaborn as sns
import statsmodels.api as sm
import matplotlib.pyplot as plt
import matplotlib.pylab as pylab
%matplotlib inline
from itertools import product

from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score

from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

torch.manual_seed(0);
```


```python
# Hyper Parameters

obs_point = 921
region = 'bu'
vis_limit_list = [1000, 3000,5000,10000,30000]

num_unit_1_list = [256]
num_unit_2_list = [256]
num_unit_3_list = [64]
lr_list = [0.0004]
batch_size_list = [32]
epochs_list = [300,800,1300,1800]
k_fold_list = [5]

data_path = 'D:\\Onedrive\\CNU\\OneDrive - 충남대학교\\Coding\\LAB\\Lab\\preprocessing_templete(for fog data)\\data\\new_fog_data\\' + str(obs_point)
files = glob.glob(data_path + '\\*.csv')

def parameters_list(num_unit_1_list, num_unit_2_list, num_unit_3_list, lr_list, batch_size_list, epochs_list, k_fold_list):
    items = [num_unit_1_list, num_unit_2_list, num_unit_3_list, lr_list, batch_size_list, epochs_list, k_fold_list]
    result = list(product(*items))
    return result

all_parameters_list = parameters_list(num_unit_1_list, num_unit_2_list, num_unit_3_list, lr_list, batch_size_list, epochs_list, k_fold_list)
```

# DNN Network Clss


```python
class Deep_Neural_Network(nn.Module):
    def __init__(self, num_unit_1 = 200, num_unit_2 = 70, num_unit_3 = 20, drop_out=0):
        super(Deep_Neural_Network, self).__init__()
        self.fc1 = nn.Linear(9, num_unit_1)
        self.fc2 = nn.Linear(num_unit_1, num_unit_1)
        self.fc3 = nn.Linear(num_unit_1, num_unit_2)
        self.fc4 = nn.Linear(num_unit_2, num_unit_3)
        self.output = nn.Linear(num_unit_3, 1)
        self.dropout = nn.Dropout(drop_out)

    def forward(self, x, **kwargs):
        fc1 = self.dropout(F.leaky_relu(self.fc1(x), negative_slope=0.2))
        fc2 = self.dropout(F.leaky_relu(self.fc2(fc1), negative_slope=0.2))
        fc3 = self.dropout(F.leaky_relu(self.fc3(fc2), negative_slope=0.2))
        fc4 = self.dropout(F.leaky_relu(self.fc4(fc3), negative_slope=0.2))
        fc5 = self.output(fc4)
        return fc5
```

# Train Function


```python
def train(model, device, n_epochs, optimizer, loss_fn, batch_size, n_fold, kfold, torch_train_x, torch_train_y, torch_test_x, torch_test_y, save_Model=False):
    total_acc = 0
    per_proceed = 0
    for fold, (train_index, test_index) in enumerate(kfold.split(torch_train_x, torch_train_y)):
        ### Dividing data into folds
        x_train_fold = torch_train_x[train_index]
        x_test_fold = torch_train_x[test_index]
        y_train_fold = torch_train_y[train_index]
        y_test_fold = torch_train_y[test_index]

        train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)
        test = torch.utils.data.TensorDataset(x_test_fold, y_test_fold)
        train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)
        test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)

        for epoch in range(n_epochs):
            # print('\nEpoch {} / {} \nFold number {} / {}'.format(epoch + 1, epochs, fold + 1 , kfold.get_n_splits()))
            # network.train()
            for batch_index, (x_batch, y_batch) in enumerate(train_loader):
                x_batch, y_batch = x_batch.to(device), y_batch.to(device)
                optimizer.zero_grad()
                # print(x_batch.shape)
                out = model(x_batch)
                y_batch = y_batch.reshape([y_batch.shape[0],1])
                # print(out.shape, y_batch.shape)
                loss = loss_fn(out, y_batch)
                # loss = torch.tensor(torch.round(loss), dtype=torch.float, requires_grad=True)
                # print(loss)
                loss.backward()
                optimizer.step()

            train_r2_score = r2_score(np.array(torch_train_y), model(torch_train_x.cuda()).cpu().detach().numpy())
            train_r_score = return_corr(model, torch_train_x, torch_train_y)
            train_df = pd.DataFrame({'Target':np.array(torch_train_y),'Model':model(torch_train_x.cuda()).cpu().detach().numpy().reshape(-1)})
            train_rmse = sqrt(mean_squared_error(np.array(torch_train_y), model(torch_train_x.cuda()).cpu().detach().numpy()))

            test_r2_score = r2_score(np.array(torch_test_y), model(torch_test_x.cuda()).cpu().detach().numpy())
            test_r_score = return_corr(model, torch_test_x, torch_test_y)
            test_df = pd.DataFrame({'Target':np.array(torch_test_y),'Model':model(torch_test_x.cuda()).cpu().detach().numpy().reshape(-1)})
            test_rmse = sqrt(mean_squared_error(np.array(torch_test_y), model(torch_test_x.cuda()).cpu().detach().numpy()))

            total_result_df = pd.concat([train_df, test_df], axis = 1)
            if epoch % 2 == 0:
                per_proceed += 50
                print(" Fold : {}, Epoch : {}\ntrain r2 score : {:.3f}, train r score : {:.3f}, train_rmse : {:.2f}\ntest r2 score : {:.3f}, test r score : {:.3f}, test_rmse : {:.2f}\n\n".
                      format(fold + 1, epoch + 1, train_r2_score, train_r_score, train_rmse,test_r2_score, test_r_score, test_rmse))
                # print(total_result_df.head(10),'\n\n')

        vali_r2_score = 0
        for vali_x_batch, vali_y_batch in test_loader:
            # print(vali_x_batch.shape, vali_y_batch.shape)
            vali_x_batch = vali_x_batch.to(device)
            vali_out = model(vali_x_batch)
            r2 = r2_score(vali_y_batch, vali_out.cpu().detach().numpy())
            vali_r2_score += r2

        print("vali_r2_score is {:.3f}".format(vali_r2_score / len(test_loader)))
    test_rmse = sqrt(mean_squared_error(np.array(torch_test_y), model(torch_test_x.cuda()).cpu().detach().numpy()))

    print("Fold : {}, Epoch : {}\n train r2 score : {:.3f}, train r score : {:.3f}, train_rmse : {:.2f}\ntest r2 score : {:.3f}, test r score : {:.3f}, test_rmse : {:.2f}\n\n".
          format(fold + 1, epoch + 1, train_r2_score, train_r_score, train_rmse, test_r2_score, test_r_score, test_rmse))

    return model
```

# Preprocessing Function


```python
def minmax_func(df):
    # 타겟값 및 이미 sin변환한 데이터를 제외하고 표준화 하기
    normal_columns = df.columns.difference(['Local Time','DOY','vis'])
    minmax_df = df[normal_columns]
    scaler = MinMaxScaler(); scaler.fit(minmax_df)
    minmax_columns = 'stand_' + minmax_df.columns
    minmax_df = scaler.transform(minmax_df)
    minmax_df = pd.DataFrame(minmax_df, columns=minmax_columns)
    minmax_df = pd.concat([minmax_df, df[['Local Time','DOY','vis']].reset_index().drop(['index'],axis=1)], axis=1)
    return minmax_df


def doy_local_to_sin(data_path):
    result_pd = pd.read_csv(data_path)

    # 로컬타임 사인으로 변환한후 더하기
    local_time = result_pd['hour'] + result_pd['minute']/60
    sin_local_time = (np.sin((local_time*2*np.pi)/24))
    result_pd['Local Time'] = sin_local_time

    if result_pd['year'][0] % 4 == 0:
        result_pd['DOY'] = leap_doy
    else:
        result_pd['DOY'] = normal_doy
    return result_pd

def return_corr(model, torch_test_x, torch_test_y):
    r2_pred = model(torch_test_x.cuda()).cpu().detach().numpy()
    r2_target = np.array(torch_test_y)
    df_r2 = pd.DataFrame()
    df_r2['r2_pred'] = r2_pred[:,0]
    df_r2['r2_target'] = r2_target
    return df_r2.corr().iloc[0,1]


# oversample function
def number_of_data(df, kind_of_data, min_num, max_num):
    cond1 = df[kind_of_data] > min_num
    cond2 = df[kind_of_data] <= max_num
    return df[cond1 & cond2]

# oversample function
def append_same_data(df, time):
    result_df = pd.DataFrame()
    for i in range(time):
        result_df = result_df.append(df)
    return result_df

def convert_torch_type(input_train_data, target_train_data, input_test_data, target_test_data):
    torch_train_x = torch.tensor(np.array(input_train_data), dtype=torch.float32)
    torch_train_y = torch.tensor(np.array(target_train_data), dtype=torch.float32)
    torch_test_x = torch.tensor(np.array(input_test_data), dtype=torch.float32)
    torch_test_y = torch.tensor(np.array(target_test_data), dtype=torch.float32)
    return torch_train_x, torch_train_y, torch_test_x, torch_test_y
```

## (Preprocessing) Convert Day Of Year, Local time to sin

### - result_pd == total_pd


```python
leap_month_list = [31,29,31,30,31,30,31,31,30,31,30,31]
normal_month_list = [31,28,31,30,31,30,31,31,30,31,30,31]
leap_year_list = []
normal_year_list = []
for i in range(sum(leap_month_list)):
    temp = [i+1]*144
    leap_year_list += temp
    
for i in range(sum(normal_month_list)):
    temp = [i+1]*144
    normal_year_list += temp
    
leap_doy = np.sin((np.array(leap_year_list)*(2*np.pi))/366)
normal_doy = np.sin((np.array(normal_year_list)*(2*np.pi))/365)

for i in range(len(files)):
    if i == 0:
        total_pd = pd.read_csv(files[i])
        
        # 로컬타임 사인으로 변환한후 더하기
        local_time = total_pd['hour'] + total_pd['minute']/60
        sin_local_time = (np.sin((local_time*2*np.pi)/24))
        total_pd['Local Time'] = sin_local_time
        
        if total_pd['year'][0] % 4 == 0:
            total_pd['DOY'] = leap_doy
        else:
            total_pd['DOY'] = normal_doy
            
        
        
    else:
        temp_pd = pd.read_csv(files[i])
        
        # 로컬타임 사인으로 변환한후 더하기
        local_time = temp_pd['hour'] + temp_pd['minute']/60
        sin_local_time = (np.sin((local_time*2*np.pi)/24))
        temp_pd['Local Time'] = sin_local_time
        
        if temp_pd['year'][0] % 4 == 0:
            temp_pd['DOY'] = leap_doy
        else:
            temp_pd['DOY'] = normal_doy
        
        
        total_pd = pd.concat([total_pd, temp_pd], axis = 0)
```

## Import data & Drop useless data


```python
if region == 'bu':
    # train_pd.drop(['Unnamed: 0','year','month','day','hour','minute','Fog_10','LST','WD','RH','WS','Td','AT','WT-AT'], axis=1, inplace = True)
    total_pd.drop(['Unnamed: 0','year','month','day','hour','minute','Fog_10','LST','bWD','bRH','bWS','bTd','bAT','WT-AT'], axis=1, inplace = True)
    total_pd.replace(-999, np.nan, inplace = True)
    total_pd.columns = ['Fog_30', 'ww', 'vis', 'RN', 'WT', 'AT', 'RH', 'WS', 'WD', 'Td', 'AT-3hr', 'Local Time', 'DOY']

elif region == 'se':
    total_pd.drop(['Unnamed: 0','year','month','day','hour','minute','Fog_10','LST','WT-AT'], axis=1, inplace = True)
    total_pd.replace(-999, np.nan, inplace = True)

```


```python
## Remove null & Calculate 'WT-AT'
```


```python
# Remove null value
total_data = total_pd.dropna()
print(total_data.shape)


# 'WT-AT'
total_data['WT-AT'] = total_data['WT'] - total_data['AT']
```

# Choose method(only one)

### 1st method : remove non occurrence fog data

### 2nd method : limit visibility
- 3000 -> 64
- 5000 -> 128
- 10000 -> 256
- 30000 -> 2048


```python
# 1st method
total_df = total_data[total_data['Fog_30'] == 1]
```


```python
# 2nd method
# vis_limit_num = 5000
# total_df = total_data[total_data['vis'] < vis_limit_num]
```

## Split data for train, test


```python
# train test split
X_train_columns = scaled_total_df.columns.difference(['stand_RN','stand_AT-3hr','stand_ww',
                                                     'stand_Fog_30','vis'])

input_data = scaled_total_df[X_train_columns]
train_x, test_x, train_y, test_y = train_test_split(input_data, (scaled_total_df['vis']).astype(np.int),
                                    test_size = 0.3, random_state=42)
print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)
```

    (11221, 9) (11221,) (4810, 9) (4810,)
    

## array to torch(for train)


```python
# change to torch type
torch_train_x, torch_train_y, torch_test_x, torch_test_y = convert_torch_type(train_x, train_y, test_x, test_y)
torch_train_x = torch_train_x.reshape([torch_train_x.shape[0], torch_train_x.shape[1]])
torch_test_x = torch_test_x.reshape([torch_test_x.shape[0], torch_test_x.shape[1]])
print(torch_train_x.shape, torch_train_y.shape, torch_test_x.shape, torch_test_y.shape)
```

    torch.Size([11221, 9]) torch.Size([11221]) torch.Size([4810, 9]) torch.Size([4810])
    

## Train Code


```python
train_r_result = []
train_r2_result = []
train_rmse_result = []
test_r_result = []
test_r2_result = []
test_rmse_result = []
used_time_list = []

for i in range(len(all_parameters_list)):
    print("진행률 : {}%".format(((i + 1) / len(all_parameters_list)) * 100))
    start = time.time()
    num_unit_1, num_unit_2, num_unit_3, lr, batch_size, n_epochs, n_fold,  = all_parameters_list[i][0], \
                                                                                      all_parameters_list[i][1], \
                                                                                      all_parameters_list[i][2], \
                                                                                      all_parameters_list[i][3], \
                                                                                      all_parameters_list[i][4], \
                                                                                      all_parameters_list[i][5], \
                                                                                      all_parameters_list[i][6],
    seed = 1
    lr = lr
    momentum = 0.5
    no_cuda = False
    batch_size = batch_size
    # 배치사이즈와 모멘텀등 몇가지 변수들을 지정해줍니다.
    torch.manual_seed(seed)
    use_cuda = not no_cuda and torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    model = Deep_Neural_Network(num_unit_1=num_unit_1, num_unit_2=num_unit_2, num_unit_3=num_unit_3).to(device)
    optimizer = optim.Adam(model.parameters(), lr=lr)
    loss_fn = torch.nn.MSELoss(reduction='sum')
    kfold = KFold(n_splits=n_fold, shuffle=False)
    network = train(model, device, n_epochs, optimizer, loss_fn, batch_size, n_fold, kfold, torch_train_x, torch_train_y, torch_test_x, torch_test_y)


    used_time = time.time() - start
    used_time_list.append(used_time)
    print("현재환경", num_unit_1, num_unit_2, num_unit_3, lr, batch_size, n_epochs, n_fold)
    print("총 걸린시간은 {}초 입니다.\n".format(used_time))
    
    
    # train, test DataFrame
    train_df = pd.DataFrame({'Target': np.array(torch_train_y),
                             'Model': network(torch_train_x.cuda()).cpu().detach().numpy().reshape(-1)})
    
    
    test_df = pd.DataFrame({'Target': np.array(torch_test_y),
                            'Model': network(torch_test_x.cuda()).cpu().detach().numpy().reshape(-1)})
    
    # calculate model result
    train_r_score = np.array(train_df.corr())[0][1]
    test_r_score = np.array(test_df.corr())[0][1]
    train_r2_score = r2_score(np.array(torch_train_y), network(torch_train_x.cuda()).cpu().detach().numpy().reshape(-1))
    test_r2_score = r2_score(np.array(torch_test_y), network(torch_test_x.cuda()).cpu().detach().numpy().reshape(-1))
    train_rmse = sqrt(mean_squared_error(np.array(torch_train_y), model(torch_train_x.cuda()).cpu().detach().numpy()))
    test_rmse = sqrt(mean_squared_error(np.array(torch_test_y), model(torch_test_x.cuda()).cpu().detach().numpy()))
    
    train_r_result.append(train_r_score)
    test_r_result.append(test_r_score)
    train_r2_result.append(train_r2_score)
    test_r2_result.append(test_r2_score)
    train_rmse_result.append(train_rmse)
    test_rmse_result.append(test_rmse)
    
    
    # Drawing Scatter Plot
    x = train_df['Target']
    y = train_df['Model']
    z = np.polyfit(x,y,1)
    p = np.poly1d(z)

    plt.rcParams['font.family'] = "Malgun Gothic"
    plt.rcParams['font.size'] = 30
    plt.rcParams['figure.figsize'] = (20, 20)
    pylab.plot(x,y,'o')
    pylab.plot(x,p(x),"r--", linewidth = 3)
    plt.title('(Train) '+str(obs_point)+' under {}'.format(vis_limit_num), pad = 30, fontdict={'size':40})
    plt.xlim(0,vis_limit_num)
    plt.ylim(0,vis_limit_num)
    plt.xlabel("(Train) Target")
    plt.ylabel("(Train) Model")
    plt.text(vis_limit_num//50, vis_limit_num + vis_limit_num//50, 'R : {:.3f} / R2 : {:.3f}'.format(train_r_score, train_r2_score), bbox=box1, fontdict = font2)
    plt.text(vis_limit_num//50, vis_limit_num - vis_limit_num//30, "y = %.3fx + (%.3f)"%(z[0],z[1]), bbox=box1, fontdict = font2)
    plt.savefig('D:\\result\\fog_result\\DNN\\'+
            'Train_{}_under_{}_{}_{}_{}_{}_{}_{}_{}.jpg'.format(obs_point,vis_limit_num, num_unit_1, num_unit_2, num_unit_3, lr, batch_size, n_epochs, n_fold), dpi = 300)
    plt.show()
    plt.clf()

    
    x = test_df['Target']
    y = test_df['Model']
    z = np.polyfit(x,y,1)
    p = np.poly1d(z)

    plt.rcParams['font.family'] = "Malgun Gothic"
    plt.rcParams['font.size'] = 30
    plt.rcParams['figure.figsize'] = (20, 20)
    pylab.plot(x,y,'o')
    pylab.plot(x,p(x),"r--", linewidth = 3)
    plt.title('(Test) '+str(obs_point)+' under {}'.format(vis_limit_num), pad = 30, fontdict={'size':40})
    plt.xlim(0,vis_limit_num)
    plt.ylim(0,vis_limit_num)
    plt.xlabel("(Test) Target")
    plt.ylabel("(Test) Model")    
    plt.text(vis_limit_num//50, vis_limit_num + vis_limit_num//50, 'R : {:.3f} / R2 : {:.3f}'.format(test_r_score, test_r2_score), bbox=box1, fontdict = font2)
    plt.text(vis_limit_num//50, vis_limit_num - vis_limit_num//30, "y = %.3fx + (%.3f)"%(z[0],z[1]), bbox=box1, fontdict = font2)
    plt.savefig('D:\\result\\fog_result\\DNN\\'+
                'Test_{}_under_{}_{}_{}_{}_{}_{}_{}_{}.jpg'.format(obs_point,vis_limit_num, num_unit_1, num_unit_2, num_unit_3, lr, batch_size, n_epochs, n_fold), dpi = 300)
    plt.show()
    plt.clf()
```

    진행률 : 25.0%
     Fold : 1, Epoch : 1
    train r2 score : -0.057, train r score : -0.158, train_rmse : 1501.21
    test r2 score : -0.050, test r score : -0.130, test_rmse : 1494.07
    
    
     Fold : 1, Epoch : 3
    train r2 score : -0.023, train r score : -0.053, train_rmse : 1476.74
    test r2 score : -0.019, test r score : -0.033, test_rmse : 1471.86
    
    
     Fold : 1, Epoch : 5
    train r2 score : 0.008, train r score : 0.106, train_rmse : 1454.36
    test r2 score : 0.006, test r score : 0.107, test_rmse : 1453.60
    
    
     Fold : 1, Epoch : 7
    train r2 score : 0.044, train r score : 0.218, train_rmse : 1427.27
    test r2 score : 0.042, test r score : 0.207, test_rmse : 1427.27
    
    
     Fold : 1, Epoch : 9
    train r2 score : 0.054, train r score : 0.277, train_rmse : 1420.37
    test r2 score : 0.048, test r score : 0.257, test_rmse : 1422.12
    
    
     Fold : 1, Epoch : 11
    train r2 score : 0.091, train r score : 0.304, train_rmse : 1392.19
    test r2 score : 0.079, test r score : 0.283, test_rmse : 1398.88
    
    
     Fold : 1, Epoch : 13
    train r2 score : 0.091, train r score : 0.327, train_rmse : 1392.29
    test r2 score : 0.072, test r score : 0.305, test_rmse : 1404.70
    
    
     Fold : 1, Epoch : 15
    train r2 score : 0.097, train r score : 0.334, train_rmse : 1387.76
    test r2 score : 0.088, test r score : 0.316, test_rmse : 1392.18
    
    
     Fold : 1, Epoch : 17
    train r2 score : 0.121, train r score : 0.350, train_rmse : 1368.69
    test r2 score : 0.104, test r score : 0.327, test_rmse : 1380.24
    
    
     Fold : 1, Epoch : 19
    train r2 score : 0.111, train r score : 0.351, train_rmse : 1376.38
    test r2 score : 0.092, test r score : 0.330, test_rmse : 1389.35
    
    
     Fold : 1, Epoch : 21
    train r2 score : 0.127, train r score : 0.358, train_rmse : 1363.82
    test r2 score : 0.109, test r score : 0.331, test_rmse : 1376.06
    
    
     Fold : 1, Epoch : 23
    train r2 score : 0.126, train r score : 0.357, train_rmse : 1364.61
    test r2 score : 0.108, test r score : 0.335, test_rmse : 1376.54
    
    
     Fold : 1, Epoch : 25
    train r2 score : 0.130, train r score : 0.362, train_rmse : 1362.14
    test r2 score : 0.109, test r score : 0.335, test_rmse : 1376.26
    
    
     Fold : 1, Epoch : 27
    train r2 score : 0.130, train r score : 0.361, train_rmse : 1361.60
    test r2 score : 0.112, test r score : 0.336, test_rmse : 1373.94
    
    
     Fold : 1, Epoch : 29
    train r2 score : 0.121, train r score : 0.362, train_rmse : 1368.78
    test r2 score : 0.106, test r score : 0.337, test_rmse : 1378.56
    
    
     Fold : 1, Epoch : 31
    train r2 score : 0.134, train r score : 0.366, train_rmse : 1358.74
    test r2 score : 0.113, test r score : 0.338, test_rmse : 1372.78
    
    
     Fold : 1, Epoch : 33
    train r2 score : 0.128, train r score : 0.364, train_rmse : 1363.59
    test r2 score : 0.109, test r score : 0.337, test_rmse : 1375.87
    
    
     Fold : 1, Epoch : 35
    train r2 score : 0.134, train r score : 0.367, train_rmse : 1358.27
    test r2 score : 0.112, test r score : 0.337, test_rmse : 1373.65
    
    
     Fold : 1, Epoch : 37
    train r2 score : 0.131, train r score : 0.371, train_rmse : 1360.99
    test r2 score : 0.107, test r score : 0.342, test_rmse : 1377.82
    
    
     Fold : 1, Epoch : 39
    train r2 score : 0.134, train r score : 0.372, train_rmse : 1358.39
    test r2 score : 0.110, test r score : 0.342, test_rmse : 1375.28
    
    
     Fold : 1, Epoch : 41
    train r2 score : 0.134, train r score : 0.369, train_rmse : 1358.91
    test r2 score : 0.111, test r score : 0.343, test_rmse : 1374.25
    
    
     Fold : 1, Epoch : 43
    train r2 score : 0.138, train r score : 0.373, train_rmse : 1355.24
    test r2 score : 0.117, test r score : 0.343, test_rmse : 1370.05
    
    
     Fold : 1, Epoch : 45
    train r2 score : 0.141, train r score : 0.375, train_rmse : 1353.38
    test r2 score : 0.117, test r score : 0.345, test_rmse : 1369.71
    
    
     Fold : 1, Epoch : 47
    train r2 score : 0.117, train r score : 0.373, train_rmse : 1371.60
    test r2 score : 0.088, test r score : 0.340, test_rmse : 1392.62
    
    
     Fold : 1, Epoch : 49
    train r2 score : 0.118, train r score : 0.371, train_rmse : 1371.26
    test r2 score : 0.094, test r score : 0.333, test_rmse : 1387.65
    
    
     Fold : 1, Epoch : 51
    train r2 score : 0.140, train r score : 0.377, train_rmse : 1353.89
    test r2 score : 0.119, test r score : 0.347, test_rmse : 1368.66
    
    
     Fold : 1, Epoch : 53
    train r2 score : 0.131, train r score : 0.382, train_rmse : 1361.31
    test r2 score : 0.100, test r score : 0.347, test_rmse : 1383.11
    
    
     Fold : 1, Epoch : 55
    train r2 score : 0.132, train r score : 0.382, train_rmse : 1360.46
    test r2 score : 0.104, test r score : 0.350, test_rmse : 1380.30
    
    
     Fold : 1, Epoch : 57
    train r2 score : 0.126, train r score : 0.380, train_rmse : 1364.63
    test r2 score : 0.106, test r score : 0.348, test_rmse : 1378.70
    
    
     Fold : 1, Epoch : 59
    train r2 score : 0.148, train r score : 0.386, train_rmse : 1347.42
    test r2 score : 0.120, test r score : 0.349, test_rmse : 1367.56
    
    
     Fold : 1, Epoch : 61
    train r2 score : 0.145, train r score : 0.385, train_rmse : 1350.37
    test r2 score : 0.121, test r score : 0.353, test_rmse : 1366.85
    
    
     Fold : 1, Epoch : 63
    train r2 score : 0.148, train r score : 0.390, train_rmse : 1347.75
    test r2 score : 0.119, test r score : 0.354, test_rmse : 1368.58
    
    
     Fold : 1, Epoch : 65
    train r2 score : 0.153, train r score : 0.392, train_rmse : 1343.56
    test r2 score : 0.124, test r score : 0.354, test_rmse : 1364.65
    
    
     Fold : 1, Epoch : 67
    train r2 score : 0.139, train r score : 0.389, train_rmse : 1354.61
    test r2 score : 0.114, test r score : 0.352, test_rmse : 1372.32
    
    
     Fold : 1, Epoch : 69
    train r2 score : 0.131, train r score : 0.393, train_rmse : 1360.83
    test r2 score : 0.105, test r score : 0.353, test_rmse : 1379.07
    
    
     Fold : 1, Epoch : 71
    train r2 score : 0.157, train r score : 0.397, train_rmse : 1340.27
    test r2 score : 0.125, test r score : 0.356, test_rmse : 1363.50
    
    
     Fold : 1, Epoch : 73
    train r2 score : 0.148, train r score : 0.396, train_rmse : 1347.52
    test r2 score : 0.113, test r score : 0.355, test_rmse : 1373.37
    
    
     Fold : 1, Epoch : 75
    train r2 score : 0.155, train r score : 0.401, train_rmse : 1342.05
    test r2 score : 0.125, test r score : 0.360, test_rmse : 1363.82
    
    
     Fold : 1, Epoch : 77
    train r2 score : 0.161, train r score : 0.402, train_rmse : 1337.60
    test r2 score : 0.131, test r score : 0.365, test_rmse : 1358.87
    
    
     Fold : 1, Epoch : 79
    train r2 score : 0.160, train r score : 0.403, train_rmse : 1337.81
    test r2 score : 0.125, test r score : 0.361, test_rmse : 1363.97
    
    
     Fold : 1, Epoch : 81
    train r2 score : 0.159, train r score : 0.403, train_rmse : 1339.16
    test r2 score : 0.122, test r score : 0.361, test_rmse : 1366.17
    
    
     Fold : 1, Epoch : 83
    train r2 score : 0.167, train r score : 0.411, train_rmse : 1332.50
    test r2 score : 0.130, test r score : 0.367, test_rmse : 1359.49
    
    
     Fold : 1, Epoch : 85
    train r2 score : 0.159, train r score : 0.404, train_rmse : 1338.73
    test r2 score : 0.122, test r score : 0.355, test_rmse : 1365.78
    
    
     Fold : 1, Epoch : 87
    train r2 score : 0.169, train r score : 0.414, train_rmse : 1330.95
    test r2 score : 0.130, test r score : 0.367, test_rmse : 1359.88
    
    
     Fold : 1, Epoch : 89
    train r2 score : 0.173, train r score : 0.419, train_rmse : 1327.73
    test r2 score : 0.136, test r score : 0.372, test_rmse : 1355.19
    
    
     Fold : 1, Epoch : 91
    train r2 score : 0.162, train r score : 0.422, train_rmse : 1336.66
    test r2 score : 0.125, test r score : 0.373, test_rmse : 1363.73
    
    
     Fold : 1, Epoch : 93
    train r2 score : 0.175, train r score : 0.418, train_rmse : 1326.21
    test r2 score : 0.133, test r score : 0.370, test_rmse : 1357.12
    
    
     Fold : 1, Epoch : 95
    train r2 score : 0.161, train r score : 0.423, train_rmse : 1337.11
    test r2 score : 0.125, test r score : 0.375, test_rmse : 1364.11
    
    
     Fold : 1, Epoch : 97
    train r2 score : 0.176, train r score : 0.425, train_rmse : 1325.30
    test r2 score : 0.138, test r score : 0.377, test_rmse : 1353.65
    
    
     Fold : 1, Epoch : 99
    train r2 score : 0.180, train r score : 0.428, train_rmse : 1321.97
    test r2 score : 0.137, test r score : 0.375, test_rmse : 1354.18
    
    
     Fold : 1, Epoch : 101
    train r2 score : 0.170, train r score : 0.434, train_rmse : 1330.13
    test r2 score : 0.128, test r score : 0.379, test_rmse : 1361.03
    
    
     Fold : 1, Epoch : 103
    train r2 score : 0.185, train r score : 0.434, train_rmse : 1318.11
    test r2 score : 0.137, test r score : 0.376, test_rmse : 1354.13
    
    
     Fold : 1, Epoch : 105
    train r2 score : 0.189, train r score : 0.441, train_rmse : 1314.69
    test r2 score : 0.144, test r score : 0.386, test_rmse : 1348.66
    
    
     Fold : 1, Epoch : 107
    train r2 score : 0.172, train r score : 0.437, train_rmse : 1328.47
    test r2 score : 0.129, test r score : 0.383, test_rmse : 1360.55
    
    
     Fold : 1, Epoch : 109
    train r2 score : 0.198, train r score : 0.446, train_rmse : 1307.52
    test r2 score : 0.150, test r score : 0.391, test_rmse : 1343.93
    
    
     Fold : 1, Epoch : 111
    train r2 score : 0.189, train r score : 0.436, train_rmse : 1314.86
    test r2 score : 0.138, test r score : 0.375, test_rmse : 1353.42
    
    
     Fold : 1, Epoch : 113
    train r2 score : 0.193, train r score : 0.445, train_rmse : 1311.19
    test r2 score : 0.139, test r score : 0.384, test_rmse : 1352.49
    
    
     Fold : 1, Epoch : 115
    train r2 score : 0.204, train r score : 0.453, train_rmse : 1302.65
    test r2 score : 0.155, test r score : 0.395, test_rmse : 1340.35
    
    
     Fold : 1, Epoch : 117
    train r2 score : 0.194, train r score : 0.454, train_rmse : 1310.50
    test r2 score : 0.139, test r score : 0.394, test_rmse : 1352.38
    
    
     Fold : 1, Epoch : 119
    train r2 score : 0.186, train r score : 0.455, train_rmse : 1317.64
    test r2 score : 0.130, test r score : 0.396, test_rmse : 1360.07
    
    
     Fold : 1, Epoch : 121
    train r2 score : 0.205, train r score : 0.454, train_rmse : 1301.44
    test r2 score : 0.157, test r score : 0.400, test_rmse : 1338.27
    
    
     Fold : 1, Epoch : 123
    train r2 score : 0.208, train r score : 0.463, train_rmse : 1299.68
    test r2 score : 0.154, test r score : 0.399, test_rmse : 1340.59
    
    
     Fold : 1, Epoch : 125
    train r2 score : 0.214, train r score : 0.464, train_rmse : 1294.23
    test r2 score : 0.163, test r score : 0.404, test_rmse : 1334.17
    
    
     Fold : 1, Epoch : 127
    train r2 score : 0.214, train r score : 0.465, train_rmse : 1294.23
    test r2 score : 0.160, test r score : 0.404, test_rmse : 1336.42
    
    
     Fold : 1, Epoch : 129
    train r2 score : 0.219, train r score : 0.470, train_rmse : 1290.44
    test r2 score : 0.165, test r score : 0.408, test_rmse : 1332.04
    
    
     Fold : 1, Epoch : 131
    train r2 score : 0.206, train r score : 0.460, train_rmse : 1301.36
    test r2 score : 0.144, test r score : 0.395, test_rmse : 1348.87
    
    
     Fold : 1, Epoch : 133
    train r2 score : 0.223, train r score : 0.475, train_rmse : 1286.92
    test r2 score : 0.164, test r score : 0.409, test_rmse : 1332.93
    
    
     Fold : 1, Epoch : 135
    train r2 score : 0.217, train r score : 0.466, train_rmse : 1292.27
    test r2 score : 0.157, test r score : 0.399, test_rmse : 1338.92
    
    
     Fold : 1, Epoch : 137
    train r2 score : 0.211, train r score : 0.461, train_rmse : 1296.73
    test r2 score : 0.160, test r score : 0.408, test_rmse : 1336.21
    
    
     Fold : 1, Epoch : 139
    train r2 score : 0.222, train r score : 0.475, train_rmse : 1287.94
    test r2 score : 0.166, test r score : 0.415, test_rmse : 1331.26
    
    
     Fold : 1, Epoch : 141
    train r2 score : 0.180, train r score : 0.481, train_rmse : 1322.22
    test r2 score : 0.124, test r score : 0.414, test_rmse : 1364.82
    
    
     Fold : 1, Epoch : 143
    train r2 score : 0.227, train r score : 0.485, train_rmse : 1283.79
    test r2 score : 0.167, test r score : 0.422, test_rmse : 1330.70
    
    
     Fold : 1, Epoch : 145
    train r2 score : 0.233, train r score : 0.488, train_rmse : 1278.95
    test r2 score : 0.172, test r score : 0.424, test_rmse : 1326.69
    
    
     Fold : 1, Epoch : 147
    train r2 score : 0.235, train r score : 0.485, train_rmse : 1277.12
    test r2 score : 0.177, test r score : 0.422, test_rmse : 1322.96
    
    
     Fold : 1, Epoch : 149
    train r2 score : 0.239, train r score : 0.490, train_rmse : 1273.38
    test r2 score : 0.175, test r score : 0.422, test_rmse : 1323.79
    
    
     Fold : 1, Epoch : 151
    train r2 score : 0.221, train r score : 0.470, train_rmse : 1288.95
    test r2 score : 0.161, test r score : 0.406, test_rmse : 1335.06
    
    
     Fold : 1, Epoch : 153
    train r2 score : 0.230, train r score : 0.486, train_rmse : 1281.35
    test r2 score : 0.166, test r score : 0.421, test_rmse : 1331.10
    
    
     Fold : 1, Epoch : 155
    train r2 score : 0.238, train r score : 0.488, train_rmse : 1274.30
    test r2 score : 0.175, test r score : 0.423, test_rmse : 1323.84
    
    
     Fold : 1, Epoch : 157
    train r2 score : 0.190, train r score : 0.482, train_rmse : 1314.06
    test r2 score : 0.130, test r score : 0.416, test_rmse : 1360.15
    
    
     Fold : 1, Epoch : 159
    train r2 score : 0.242, train r score : 0.495, train_rmse : 1270.78
    test r2 score : 0.176, test r score : 0.425, test_rmse : 1323.35
    
    
     Fold : 1, Epoch : 161
    train r2 score : 0.252, train r score : 0.504, train_rmse : 1262.80
    test r2 score : 0.189, test r score : 0.435, test_rmse : 1313.10
    
    
     Fold : 1, Epoch : 163
    train r2 score : 0.243, train r score : 0.494, train_rmse : 1270.51
    test r2 score : 0.179, test r score : 0.427, test_rmse : 1321.37
    
    
     Fold : 1, Epoch : 165
    train r2 score : 0.251, train r score : 0.502, train_rmse : 1263.65
    test r2 score : 0.186, test r score : 0.437, test_rmse : 1314.95
    
    
     Fold : 1, Epoch : 167
    train r2 score : 0.229, train r score : 0.504, train_rmse : 1282.10
    test r2 score : 0.161, test r score : 0.436, test_rmse : 1335.51
    
    
     Fold : 1, Epoch : 169
    train r2 score : 0.252, train r score : 0.503, train_rmse : 1262.72
    test r2 score : 0.188, test r score : 0.436, test_rmse : 1313.82
    
    
     Fold : 1, Epoch : 171
    train r2 score : 0.209, train r score : 0.508, train_rmse : 1298.85
    test r2 score : 0.148, test r score : 0.440, test_rmse : 1345.77
    
    
     Fold : 1, Epoch : 173
    train r2 score : 0.262, train r score : 0.514, train_rmse : 1254.27
    test r2 score : 0.195, test r score : 0.445, test_rmse : 1307.92
    
    
     Fold : 1, Epoch : 175
    train r2 score : 0.246, train r score : 0.502, train_rmse : 1268.09
    test r2 score : 0.181, test r score : 0.441, test_rmse : 1319.25
    
    
     Fold : 1, Epoch : 177
    train r2 score : 0.264, train r score : 0.515, train_rmse : 1252.15
    test r2 score : 0.196, test r score : 0.444, test_rmse : 1307.10
    
    
     Fold : 1, Epoch : 179
    train r2 score : 0.234, train r score : 0.512, train_rmse : 1278.06
    test r2 score : 0.162, test r score : 0.442, test_rmse : 1334.50
    
    
     Fold : 1, Epoch : 181
    train r2 score : 0.257, train r score : 0.518, train_rmse : 1258.58
    test r2 score : 0.187, test r score : 0.448, test_rmse : 1314.82
    
    
     Fold : 1, Epoch : 183
    train r2 score : 0.229, train r score : 0.509, train_rmse : 1281.95
    test r2 score : 0.163, test r score : 0.439, test_rmse : 1333.81
    
    
     Fold : 1, Epoch : 185
    train r2 score : 0.254, train r score : 0.519, train_rmse : 1260.94
    test r2 score : 0.185, test r score : 0.449, test_rmse : 1316.53
    
    
     Fold : 1, Epoch : 187
    train r2 score : 0.258, train r score : 0.517, train_rmse : 1257.91
    test r2 score : 0.194, test r score : 0.451, test_rmse : 1308.72
    
    
     Fold : 1, Epoch : 189
    train r2 score : 0.249, train r score : 0.507, train_rmse : 1265.29
    test r2 score : 0.180, test r score : 0.443, test_rmse : 1320.44
    
    
     Fold : 1, Epoch : 191
    train r2 score : 0.240, train r score : 0.506, train_rmse : 1272.96
    test r2 score : 0.170, test r score : 0.438, test_rmse : 1328.24
    
    
     Fold : 1, Epoch : 193
    train r2 score : 0.271, train r score : 0.521, train_rmse : 1246.79
    test r2 score : 0.199, test r score : 0.448, test_rmse : 1305.05
    
    
     Fold : 1, Epoch : 195
    train r2 score : 0.273, train r score : 0.524, train_rmse : 1244.97
    test r2 score : 0.202, test r score : 0.453, test_rmse : 1302.29
    
    
     Fold : 1, Epoch : 197
    train r2 score : 0.260, train r score : 0.523, train_rmse : 1256.23
    test r2 score : 0.187, test r score : 0.455, test_rmse : 1314.75
    
    
     Fold : 1, Epoch : 199
    train r2 score : 0.260, train r score : 0.532, train_rmse : 1255.98
    test r2 score : 0.185, test r score : 0.460, test_rmse : 1316.15
    
    
     Fold : 1, Epoch : 201
    train r2 score : 0.266, train r score : 0.526, train_rmse : 1250.99
    test r2 score : 0.193, test r score : 0.451, test_rmse : 1309.27
    
    
     Fold : 1, Epoch : 203
    train r2 score : 0.271, train r score : 0.534, train_rmse : 1246.92
    test r2 score : 0.198, test r score : 0.463, test_rmse : 1305.30
    
    
     Fold : 1, Epoch : 205
    train r2 score : 0.216, train r score : 0.530, train_rmse : 1293.12
    test r2 score : 0.146, test r score : 0.459, test_rmse : 1347.54
    
    
     Fold : 1, Epoch : 207
    train r2 score : 0.288, train r score : 0.540, train_rmse : 1231.69
    test r2 score : 0.214, test r score : 0.466, test_rmse : 1292.22
    
    
     Fold : 1, Epoch : 209
    train r2 score : 0.290, train r score : 0.540, train_rmse : 1230.52
    test r2 score : 0.213, test r score : 0.463, test_rmse : 1293.17
    
    
     Fold : 1, Epoch : 211
    train r2 score : 0.257, train r score : 0.537, train_rmse : 1258.78
    test r2 score : 0.185, test r score : 0.467, test_rmse : 1316.07
    
    
     Fold : 1, Epoch : 213
    train r2 score : 0.266, train r score : 0.540, train_rmse : 1250.92
    test r2 score : 0.186, test r score : 0.462, test_rmse : 1314.98
    
    
     Fold : 1, Epoch : 215
    train r2 score : 0.276, train r score : 0.526, train_rmse : 1242.42
    test r2 score : 0.196, test r score : 0.446, test_rmse : 1307.59
    
    
     Fold : 1, Epoch : 217
    train r2 score : 0.267, train r score : 0.543, train_rmse : 1250.25
    test r2 score : 0.187, test r score : 0.463, test_rmse : 1314.69
    
    
     Fold : 1, Epoch : 219
    train r2 score : 0.289, train r score : 0.538, train_rmse : 1231.35
    test r2 score : 0.213, test r score : 0.464, test_rmse : 1293.25
    
    
     Fold : 1, Epoch : 221
    train r2 score : 0.259, train r score : 0.529, train_rmse : 1256.86
    test r2 score : 0.186, test r score : 0.462, test_rmse : 1315.05
    
    
     Fold : 1, Epoch : 223
    train r2 score : 0.279, train r score : 0.546, train_rmse : 1240.02
    test r2 score : 0.202, test r score : 0.465, test_rmse : 1302.12
    
    
     Fold : 1, Epoch : 225
    train r2 score : 0.190, train r score : 0.543, train_rmse : 1314.20
    test r2 score : 0.108, test r score : 0.463, test_rmse : 1376.65
    
    
     Fold : 1, Epoch : 227
    train r2 score : 0.292, train r score : 0.543, train_rmse : 1228.37
    test r2 score : 0.209, test r score : 0.460, test_rmse : 1296.54
    
    
     Fold : 1, Epoch : 229
    train r2 score : 0.278, train r score : 0.539, train_rmse : 1240.40
    test r2 score : 0.194, test r score : 0.456, test_rmse : 1308.96
    
    
     Fold : 1, Epoch : 231
    train r2 score : 0.201, train r score : 0.514, train_rmse : 1304.98
    test r2 score : 0.134, test r score : 0.449, test_rmse : 1356.38
    
    
     Fold : 1, Epoch : 233
    train r2 score : 0.292, train r score : 0.552, train_rmse : 1228.51
    test r2 score : 0.217, test r score : 0.477, test_rmse : 1289.95
    
    
     Fold : 1, Epoch : 235
    train r2 score : 0.299, train r score : 0.552, train_rmse : 1222.22
    test r2 score : 0.216, test r score : 0.475, test_rmse : 1290.48
    
    
     Fold : 1, Epoch : 237
    train r2 score : 0.297, train r score : 0.547, train_rmse : 1224.10
    test r2 score : 0.219, test r score : 0.469, test_rmse : 1288.28
    
    
     Fold : 1, Epoch : 239
    train r2 score : 0.275, train r score : 0.545, train_rmse : 1242.86
    test r2 score : 0.198, test r score : 0.472, test_rmse : 1305.67
    
    
     Fold : 1, Epoch : 241
    train r2 score : 0.308, train r score : 0.556, train_rmse : 1214.21
    test r2 score : 0.227, test r score : 0.481, test_rmse : 1281.48
    
    
     Fold : 1, Epoch : 243
    train r2 score : 0.284, train r score : 0.549, train_rmse : 1235.59
    test r2 score : 0.200, test r score : 0.474, test_rmse : 1303.95
    
    
     Fold : 1, Epoch : 245
    train r2 score : 0.309, train r score : 0.559, train_rmse : 1213.83
    test r2 score : 0.227, test r score : 0.477, test_rmse : 1282.18
    
    
     Fold : 1, Epoch : 247
    train r2 score : 0.298, train r score : 0.549, train_rmse : 1223.44
    test r2 score : 0.219, test r score : 0.471, test_rmse : 1288.69
    
    
     Fold : 1, Epoch : 249
    train r2 score : 0.259, train r score : 0.559, train_rmse : 1257.00
    test r2 score : 0.182, test r score : 0.479, test_rmse : 1318.76
    
    
     Fold : 1, Epoch : 251
    train r2 score : 0.292, train r score : 0.544, train_rmse : 1228.68
    test r2 score : 0.203, test r score : 0.461, test_rmse : 1301.39
    
    
     Fold : 1, Epoch : 253
    train r2 score : 0.233, train r score : 0.552, train_rmse : 1278.41
    test r2 score : 0.148, test r score : 0.475, test_rmse : 1346.06
    
    
     Fold : 1, Epoch : 255
    train r2 score : 0.105, train r score : 0.547, train_rmse : 1381.42
    test r2 score : 0.017, test r score : 0.463, test_rmse : 1445.73
    
    
     Fold : 1, Epoch : 257
    train r2 score : 0.289, train r score : 0.563, train_rmse : 1230.99
    test r2 score : 0.198, test r score : 0.478, test_rmse : 1305.29
    
    
     Fold : 1, Epoch : 259
    train r2 score : 0.289, train r score : 0.542, train_rmse : 1231.05
    test r2 score : 0.204, test r score : 0.460, test_rmse : 1301.03
    
    
     Fold : 1, Epoch : 261
    train r2 score : 0.314, train r score : 0.562, train_rmse : 1209.64
    test r2 score : 0.232, test r score : 0.482, test_rmse : 1277.93
    
    
     Fold : 1, Epoch : 263
    train r2 score : 0.296, train r score : 0.548, train_rmse : 1224.59
    test r2 score : 0.209, test r score : 0.471, test_rmse : 1296.89
    
    
     Fold : 1, Epoch : 265
    train r2 score : 0.265, train r score : 0.552, train_rmse : 1251.67
    test r2 score : 0.180, test r score : 0.474, test_rmse : 1320.24
    
    
     Fold : 1, Epoch : 267
    train r2 score : 0.306, train r score : 0.559, train_rmse : 1216.00
    test r2 score : 0.218, test r score : 0.476, test_rmse : 1289.37
    
    
     Fold : 1, Epoch : 269
    train r2 score : 0.288, train r score : 0.555, train_rmse : 1232.13
    test r2 score : 0.193, test r score : 0.468, test_rmse : 1309.68
    
    
     Fold : 1, Epoch : 271
    train r2 score : 0.312, train r score : 0.562, train_rmse : 1211.38
    test r2 score : 0.222, test r score : 0.476, test_rmse : 1285.92
    
    
    


    ---------------------------------------------------------------------------

    KeyboardInterrupt                         Traceback (most recent call last)

    <ipython-input-11-908944604a99> in <module>
         30     loss_fn = torch.nn.MSELoss(reduction='sum')
         31     kfold = KFold(n_splits=n_fold, shuffle=False)
    ---> 32     network = train(model, device, n_epochs, optimizer, loss_fn, batch_size, n_fold, kfold, torch_train_x, torch_train_y, torch_test_x, torch_test_y)
         33 
         34 
    

    <ipython-input-4-4a1b0a64cb58> in train(model, device, n_epochs, optimizer, loss_fn, batch_size, n_fold, kfold, torch_train_x, torch_train_y, torch_test_x, torch_test_y, save_Model)
         21                 optimizer.zero_grad()
         22                 # print(x_batch.shape)
    ---> 23                 out = model(x_batch)
         24                 y_batch = y_batch.reshape([y_batch.shape[0],1])
         25                 # print(out.shape, y_batch.shape)
    

    ~\.conda\envs\hyun_env\lib\site-packages\torch\nn\modules\module.py in _call_impl(self, *input, **kwargs)
        887             result = self._slow_forward(*input, **kwargs)
        888         else:
    --> 889             result = self.forward(*input, **kwargs)
        890         for hook in itertools.chain(
        891                 _global_forward_hooks.values(),
    

    <ipython-input-3-0e80ac377a0f> in forward(self, x, **kwargs)
         10 
         11     def forward(self, x, **kwargs):
    ---> 12         fc1 = self.dropout(F.leaky_relu(self.fc1(x), negative_slope=0.2))
         13         fc2 = self.dropout(F.leaky_relu(self.fc2(fc1), negative_slope=0.2))
         14         fc3 = self.dropout(F.leaky_relu(self.fc3(fc2), negative_slope=0.2))
    

    ~\.conda\envs\hyun_env\lib\site-packages\torch\nn\modules\module.py in _call_impl(self, *input, **kwargs)
        887             result = self._slow_forward(*input, **kwargs)
        888         else:
    --> 889             result = self.forward(*input, **kwargs)
        890         for hook in itertools.chain(
        891                 _global_forward_hooks.values(),
    

    ~\.conda\envs\hyun_env\lib\site-packages\torch\nn\modules\dropout.py in forward(self, input)
         56 
         57     def forward(self, input: Tensor) -> Tensor:
    ---> 58         return F.dropout(input, self.p, self.training, self.inplace)
         59 
         60 
    

    ~\.conda\envs\hyun_env\lib\site-packages\torch\nn\functional.py in dropout(input, p, training, inplace)
       1074     if p < 0.0 or p > 1.0:
       1075         raise ValueError("dropout probability has to be between 0 and 1, " "but got {}".format(p))
    -> 1076     return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
       1077 
       1078 
    

    KeyboardInterrupt: 


## Save Model Result


```python
result_df = pd.DataFrame({'Train R':train_r_result, 'Train R2':train_r2_result, 'Train RMSE':train_rmse_result,'Test R':test_r_result,'Test R2':test_r2_result,'Test RMSE':test_rmse_result})
columns = ['layer_1', 'layer_2', 'layer_3', 'lr', 'bs', 'epochs', 'kfold']
nn_structure = pd.DataFrame(all_com_list, columns=columns)
result_df = pd.concat([result_df, nn_structure], axis=1)
sort_result_df = result_df.sort_values('Test R2', ascending=False)
file_name = str(obs_point) + "_minmax_random_train_exist_3" + ".csv"
# sort_result_df.to_csv('D:\\Onedrive\\CNU\\OneDrive - 충남대학교\\LAB\\과제\\스마트시티 안개\\결과정리\\상관관계 및 DNN 결과\\DNN, VISIBILITY MODEL\\' + file_name)
```
